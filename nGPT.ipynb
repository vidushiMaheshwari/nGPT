{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPnritO5ODACQpr11qdJe+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidushiMaheshwari/nGPT/blob/main/nGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchao"
      ],
      "metadata": {
        "id": "6-bG8kj_0HB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtune"
      ],
      "metadata": {
        "id": "LYJlYx1zzuu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9PrvIscxdMo2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtune.modules import RotaryPositionalEmbeddings"
      ],
      "metadata": {
        "id": "O79SnI7Bzc09"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Layers\n",
        "1. `Norm`: Used to ensure that `NLinear`'s backpropagation always returns a normalized weight matrix. the wight parameter is registered with this layer's forward function.\n",
        "2. `NLinear`: Linear layer with the constraint that its weight matrix will always be normalized.\n",
        "3. `NFeedForward`: Feedforward network with two parallel channels for gated activation. Uses SiLU non linearity on output of one gate and multiplies it by the output of the other. All weights are handled by `NLinear` and the intermediate outputs are normalized.\n",
        "3. `Scale`: Layer with scaling factor as a trainable parameter. Used for mimicking LERP's eigen values as well as scaling query and key matrices in Attention layer\n",
        "4. `NAttention`: Classic attention layer with added constraint that all Embedding and Weight matrices are `NLinear` and all intermediate outputs are normalized."
      ],
      "metadata": {
        "id": "rfswA_aqiinq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Norm(nn.Module):\n",
        "  def __init__(self, norm_dim=-1) -> None:\n",
        "    super().__init__()\n",
        "    self.norm_dim = norm_dim\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.normalize(x, p=2, dim=self.norm_dim)"
      ],
      "metadata": {
        "id": "8XxTe1NeeJNr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLinear(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out, norm_dim=-1) -> None:\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(dim_in, dim_out, bias=False)\n",
        "\n",
        "    # To ensure that the backpropagation returns a normalized matrix, I need to\n",
        "    # register the backprop of linear layer's weights\n",
        "\n",
        "    nn.utils.parametrize.register_parametrization(\n",
        "        self.linear,\n",
        "        \"weight\",\n",
        "        Norm(norm_dim)\n",
        "    )\n",
        "\n",
        "    # The random initialized weights of the linear matrix should be normed even right now\n",
        "    self.norm_weights_init_(norm_dim)\n",
        "\n",
        "  @torch.no_grad\n",
        "  def norm_weights_init_(self, norm_dim):\n",
        "    self.linear.parametrizations.weight.original.copy_(self.linear.weight)\n",
        "    # print(torch.norm(self.linear.weight, dim=norm_dim, p=2))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.normalize(self.linear(x), p=2, dim=-1)\n"
      ],
      "metadata": {
        "id": "bcz3ZSe_dQYL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scale(nn.Module):\n",
        "  # Scaling is done so that we can still use the existing non linearities (like SiLU --\n",
        "  # without scaling these non linearities don't have much to work with)\n",
        "\n",
        "  # s_a is a trainable vector with two scalars, init and scale. The initial value\n",
        "  # is scale and its value is restored in forward by multiplying init / scale\n",
        "  def __init__(self, dim_in, scale, init):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(dim_in) * scale)\n",
        "    self.init = init/scale # This is a constant value\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x * self.scale * self.init"
      ],
      "metadata": {
        "id": "obHPLLjEgYqM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NFeedForward(nn.Module):\n",
        "  # This is a normal feedfoward with two parallel channels and gated activation\n",
        "  def __init__(self, dim_in, dim_hidden, scale_gated=1.0, scale_hidden=1.0, scale_gated_init=1.0, scale_hidden_init=1.0) -> None:\n",
        "    super().__init__()\n",
        "    self._linear_hidden = NLinear(dim_in, dim_hidden)\n",
        "    self._linear_gated = NLinear(dim_in, dim_hidden)\n",
        "    self._scale_hidden = Scale(dim_hidden, scale_gated, scale_gated_init)\n",
        "    self._scale_gated = Scale(dim_hidden, scale_hidden * (dim_in ** 0.5), scale_hidden_init)\n",
        "    self._linear_out = NLinear(dim_hidden, dim_in)\n",
        "\n",
        "  def forward(self, x):\n",
        "    u = self._linear_hidden(x)\n",
        "    v = self._linear_gated(x)\n",
        "    u = self._scale_hidden(u)\n",
        "    v = self._scale_gated(v)\n",
        "    non_linearity = F.silu(u) * v\n",
        "    return self._linear_out(non_linearity)\n"
      ],
      "metadata": {
        "id": "3e5XdGrZjMFi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NAttention(nn.Module):\n",
        "  def __init__(self, dim, n_heads=8, dim_head=64, max_seq_length=512, s_qk_init=1.0, s_qk_scale = 1.0, is_causal=False) -> None:\n",
        "    # injection of positional information by RoPE distorts q and k. We propose to\n",
        "    # additionally normalize q and k, ensuring that the dot product of every query and key is under control\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.dim_head = dim_head\n",
        "\n",
        "    dim_out = dim_head * n_heads\n",
        "    self._linear_q = NLinear(dim, dim_out)\n",
        "    self._linear_k = NLinear(dim, dim_out)\n",
        "    self._linear_v = NLinear(dim, dim_out)\n",
        "\n",
        "    self.rope = RotaryPositionalEmbeddings(dim_head, max_seq_length)\n",
        "\n",
        "    self._scale_qk = Scale(dim_out, s_qk_scale, s_qk_init) # Maybe q & k should have separate layers but I don't see a reason why/ why not\n",
        "     # (The paper says that there is no need for separate k & q scaling values)\n",
        "\n",
        "     # TODO: However, there should be separate k & q per head!\n",
        "\n",
        "    self._linear_out = NLinear(dim_out, dim)\n",
        "\n",
        "    self.softmax_scale = dim_head ** 0.5\n",
        "    self.is_causal = is_causal\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    return x.view(x.shape[0], -1, self.n_heads, self.dim_head).transpose(1, 2)\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size, n_heads, seq_length, dim_head = x.shape\n",
        "    x = x.transpose(1, 2).contiguous()\n",
        "    return x.view(batch_size, seq_length, n_heads * dim_head)\n",
        "\n",
        "  def forward(self, x):\n",
        "    k, q, v = self._linear_k(x), self._linear_q(x), self._linear_v(x)\n",
        "\n",
        "    k = self._scale_qk(k)\n",
        "    q = self._scale_qk(q)\n",
        "\n",
        "    k, q, v = self.split_heads(k), self.split_heads(q), self.split_heads(v)\n",
        "\n",
        "    # Splitting destroys the norm. Let's re-norm.\n",
        "    # NOTE: In the paper there is an ablation of whether or not to normalize this, and the effects are pretty much the same\n",
        "    k = F.normalize(k, p=2, dim=-1)\n",
        "    q = F.normalize(q, p=2, dim=-1)\n",
        "\n",
        "    k, q = self.rope(k), self.rope(q)\n",
        "\n",
        "    # In traditional transformers the softmax scaling factor is 1/sqrt(d_k) because the expected variance\n",
        "    # in dot product of non-normalized key and query is d_k. In case on normalization, the expected variance\n",
        "    # is 1/d_k and so the softmax scaling factor to bring the variance to 1 should be sqrt(d_k)\n",
        "    # print(torch.norm(v, p=2, dim=-1))\n",
        "    # print(self.softmax_scale)\n",
        "\n",
        "    out = F.scaled_dot_product_attention(q, k, v, scale=self.softmax_scale, is_causal=self.is_causal)\n",
        "\n",
        "\n",
        "    # Clearly, when calculating attention, the norm will most likely not be preserved. Because it is sort of independent\n",
        "    # and tells how much each vector is similar to the other. So, we will have to normalize this output over its embedding dimension.\n",
        "    out = self.merge_heads(out)\n",
        "\n",
        "    # So what I am thinking the approach should be is to normalize the output on my own and then put it in the linear layer\n",
        "    # \"any update that causes the hidden state h to deviate from the manifold is followed by a normalization step\"\n",
        "    out = F.normalize(out, p=2, dim=-1)\n",
        "\n",
        "    # Whereas, what lucidrains did is they put in the unnormalized output in the linear layer returns that and in nTransformer\n",
        "    # they are actually normalizing the overall output of attention. I didn't like the coupling and also the shift of putting an\n",
        "    # un normalized vector into the feed forward contradicts the main idea of the paper. On the same note, the paper doesn't\n",
        "    # talk about where to normalize the output.\n",
        "    out = self._linear_out(out)\n",
        "\n",
        "    return F.normalize(out, p=2, dim=-1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vor-pVpzxJfl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Checks for NAttention"
      ],
      "metadata": {
        "id": "sJr-V6VEiUk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 1024, 512)\n",
        "x = F.normalize(x, p=2, dim=-1)"
      ],
      "metadata": {
        "id": "uWAdtj-z8jHE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_model = NAttention(dim=512)"
      ],
      "metadata": {
        "id": "3ZNKza6h89ZE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = attn_model(x)"
      ],
      "metadata": {
        "id": "0eiUZKCQ-SFS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGQ_f6I-FpBu",
        "outputId": "28d36ab8-c239-4e51-913a-fb945c4dce32"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1062, -0.0041,  0.0865,  ...,  0.0297, -0.1023, -0.0483],\n",
              "         [-0.0128, -0.0169,  0.0185,  ...,  0.0352,  0.0003, -0.0167],\n",
              "         [-0.0350, -0.0469, -0.0436,  ...,  0.0538, -0.0168, -0.0389],\n",
              "         ...,\n",
              "         [-0.0639, -0.0653,  0.0408,  ...,  0.0189, -0.0413,  0.0031],\n",
              "         [-0.0098, -0.0113,  0.0224,  ...,  0.0451, -0.0300, -0.0455],\n",
              "         [-0.0127, -0.0812, -0.0406,  ...,  0.0816, -0.0388,  0.0592]]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(x, dim=-1, p=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlIMzkG_BiMb",
        "outputId": "beca50ec-4239-4d6d-cabf-c49b660c19cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<LinalgVectorNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "831xImZ6ibyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "class nTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               dim_in,\n",
        "               dim_head=64,\n",
        "               n_heads=8,\n",
        "               max_seq_length=512,\n",
        "               depth=5,\n",
        "               is_causal=True,\n",
        "               alpha_init: int | List = 1.0,\n",
        "               alpha_ff_init: int | List = 1.0,\n",
        "               alpha_attn_init: int | List = 1.0,\n",
        "               alpha_ff_scale: int | List = 1.0,\n",
        "               alpha_attn_scale: int | List = 1.0,\n",
        "               s_qk_init: int | List = 1.0,\n",
        "               s_qk_scale: int | List = 1.0,\n",
        "               scale_gated: int | List = 1.0,\n",
        "               scale_hidden: int | List = 1.0,\n",
        "               expand_factor: int | List = 1.0,\n",
        "               scale_hidden_init: int | List = 1.0,\n",
        "               scale_gated_init: int | List = 1.0,\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    # For all the depth\n",
        "    poss_list_inputs = (\n",
        "        expand_factor, n_heads, dim_head, max_seq_length, alpha_init, alpha_ff_init, alpha_attn_init, alpha_ff_scale, s_qk_init, s_qk_scale,\n",
        "        scale_gated, scale_hidden, expand_factor, scale_hidden_init, scale_gated_init\n",
        "    )\n",
        "\n",
        "    def make_list(x):\n",
        "      if not isinstance(x, list):\n",
        "        return [x for _ in range(depth)]\n",
        "      assert len(x) == depth\n",
        "      return x\n",
        "\n",
        "    poss_list_inputs = (make_list(i) for i in poss_list_inputs)\n",
        "\n",
        "    self.layers = nn.ModuleList([])\n",
        "\n",
        "    for expand_factor, n_heads, dim_head, max_seq_length, alpha_init, alpha_ff_init, alpha_attn_init, alpha_ff_scale, s_qk_init, \\\n",
        "    s_qk_scale, scale_gated, scale_hidden, expand_factor, scale_hidden_init, scale_gated_init in zip(*poss_list_inputs):\n",
        "        attn_layer = NAttention(dim_in, n_heads, dim_head, max_seq_length, s_qk_init, s_qk_scale, is_causal)\n",
        "        attn_lerp = Scale(dim_in, alpha_attn_scale, alpha_attn_init)\n",
        "        dim_out = int(dim_in * expand_factor)\n",
        "        ff_layer = NFeedForward(dim_in, dim_out, scale_gated, scale_hidden, scale_gated_init, scale_hidden_init)\n",
        "        ff_lerp = Scale(dim_out, alpha_ff_scale, alpha_ff_init)\n",
        "        dim_in = dim_out\n",
        "        self.layers.append(nn.ModuleList([attn_layer, attn_lerp, ff_layer, ff_lerp])) # So the ModuleList actually holds like [at_1, at_lerp_1, ff_1, ff_lerp_1], [at_2, at_lerp_2, ff_2, ff_lerp_2], ...\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    for attn_layer, attn_lerp, fn_layer, fn_lerp in self.layers:\n",
        "      attn_out = attn_layer(x)\n",
        "      x = attn_lerp(attn_out - x) + x # h <- h + \\alpha_A(h_A - h)\n",
        "      x = F.normalize(x, dim=-1, p=2)\n",
        "\n",
        "      fn_out = fn_layer(x)\n",
        "      x = fn_lerp(fn_out - x) + x\n",
        "      x = F.normalize(x, dim=-1, p=2)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "mLAOD1cdqajo"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}